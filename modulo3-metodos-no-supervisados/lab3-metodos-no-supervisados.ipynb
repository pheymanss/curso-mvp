{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875c570a",
   "metadata": {},
   "source": [
    "# Laboratorio 3: Métodos de Aprendizaje No Supervisado\n",
    "\n",
    "Este laboratorio aplica técnicas de clustering y reducción de dimensionalidad a datasets reales de industria. Los ejercicios están diseñados para desarrollar competencias en segmentación de clientes, análisis exploratorio de datos de alta dimensionalidad, e interpretación de resultados para generación de insights accionables.\n",
    "\n",
    "## Objetivos del laboratorio\n",
    "\n",
    "- Aplicar K-Means para segmentación de clientes en contextos de negocio\n",
    "- Comparar diferentes números de clusters usando métricas de evaluación\n",
    "- Implementar clustering jerárquico y visualizar dendrogramas\n",
    "- Utilizar PCA para reducción de dimensionalidad y visualización\n",
    "- Aplicar técnicas modernas (t-SNE, UMAP) para exploración de datos complejos\n",
    "- Interpretar resultados y generar recomendaciones accionables\n",
    "\n",
    "## Estructura del laboratorio\n",
    "\n",
    "1. **Segmentación de clientes con K-Means**: Identificación de perfiles de consumo en una empresa de retail\n",
    "2. **Clustering jerárquico**: Análisis de jerarquías naturales en datos de productos\n",
    "3. **Reducción de dimensionalidad con PCA**: Compresión y visualización de datos de alta dimensionalidad\n",
    "4. **Técnicas avanzadas**: Comparación de t-SNE y UMAP para visualización de estructuras complejas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85337f4d",
   "metadata": {},
   "source": [
    "## Parte 1: Configuración del Entorno\n",
    "\n",
    "Importación de librerías necesarias y configuración de visualizaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c6af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerias basicas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Configuracion de visualizacion\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar tamano de figuras por defecto\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Librerías básicas importadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20db948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerias de machine learning\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "print(\"Librerías de machine learning importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98032d7b",
   "metadata": {},
   "source": [
    "## Parte 2: Segmentación de Clientes con K-Means\n",
    "\n",
    "### Contexto del negocio\n",
    "\n",
    "Una empresa de comercio electrónico desea segmentar su base de clientes para implementar estrategias de marketing diferenciadas. El objetivo es identificar grupos de clientes con comportamientos de compra similares para personalizar ofertas, optimizar campañas publicitarias, y mejorar la retención.\n",
    "\n",
    "El dataset contiene información de comportamiento de compra de 200 clientes durante el último año, incluyendo:\n",
    "- `frecuencia_compra`: Número de transacciones realizadas\n",
    "- `valor_promedio`: Monto promedio gastado por transacción (en dólares)\n",
    "- `valor_total`: Gasto total anual (en dólares)\n",
    "- `dias_ultima_compra`: Días desde la última compra (recency)\n",
    "- `productos_unicos`: Número de productos diferentes comprados\n",
    "- `tasa_devolucion`: Porcentaje de productos devueltos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6bd69a",
   "metadata": {},
   "source": [
    "### Ejercicio 2.1: Generación y exploración de datos\n",
    "\n",
    "Generar un dataset sintético de clientes y realizar un análisis exploratorio inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad60b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset de clientes\n",
    "datos_clientes = pd.read_csv('data/segmentacion_clientes_ecommerce.csv')\n",
    "\n",
    "print(f\"Dataset cargado con {len(datos_clientes)} clientes\")\n",
    "print(f\"\\nPrimeras filas del dataset:\")\n",
    "datos_clientes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787763ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar analisis exploratorio\n",
    "print(\"Estadísticas descriptivas del dataset:\\n\")\n",
    "print(datos_clientes.describe().round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Información sobre valores faltantes:\")\n",
    "print(datos_clientes.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c40284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar distribuciones de variables clave\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "variables = ['frecuencia_compra', 'valor_promedio', 'valor_total', \n",
    "             'dias_ultima_compra', 'productos_unicos', 'tasa_devolucion']\n",
    "\n",
    "for idx, var in enumerate(variables):\n",
    "    axes[idx].hist(datos_clientes[var], bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribución de {var}')\n",
    "    axes[idx].set_xlabel(var)\n",
    "    axes[idx].set_ylabel('Frecuencia')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66e9185",
   "metadata": {},
   "source": [
    "### Ejercicio 2.2: Preprocesamiento de datos\n",
    "\n",
    "La estandarización de variables es crítica en algoritmos basados en distancias como K-Means. Sin normalización, variables con escalas mayores (ej: valor_total en miles de dólares) dominan el cálculo de distancias sobre variables en escalas menores (ej: tasa_devolucion en porcentaje), sesgando los resultados. `StandardScaler` transforma cada variable a media 0 y desviación estándar 1, garantizando contribución equitativa de todas las características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781fda34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar variables para clustering (excluir cliente_id)\n",
    "variables_clustering = ['frecuencia_compra', 'valor_promedio', 'valor_total',\n",
    "                        'dias_ultima_compra', 'productos_unicos', 'tasa_devolucion']\n",
    "\n",
    "X = datos_clientes[variables_clustering]\n",
    "\n",
    "# Crear instancia de StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajustar el scaler y transformar los datos\n",
    "X_estandarizado = scaler.fit_transform(X)\n",
    "\n",
    "# Convertir a DataFrame para facilitar visualizacion\n",
    "X_std_df = pd.DataFrame(X_estandarizado, columns=variables_clustering)\n",
    "\n",
    "print(\"Datos estandarizados - primeras filas:\")\n",
    "print(X_std_df.head())\n",
    "print(\"\\nMedia de datos estandarizados (debe ser ~0):\")\n",
    "print(X_std_df.mean().round(6))\n",
    "print(\"\\nDesviación estándar de datos estandarizados (debe ser ~1):\")\n",
    "print(X_std_df.std().round(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cc8b9e",
   "metadata": {},
   "source": [
    "### Ejercicio 2.3: Método del codo\n",
    "\n",
    "El método del codo evalúa la inercia (suma de distancias cuadradas intra-cluster) para diferentes valores de k. La inercia decrece monotónicamente con k adicionales, pero la tasa de mejora disminuye. El \"codo\" del gráfico indica el punto donde clusters adicionales aportan rendimientos marginales decrecientes, sugiriendo el número óptimo para balance entre parsimonia del modelo y calidad de clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f99ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar inercia para diferentes numeros de clusters\n",
    "rango_k = range(2, 11)\n",
    "inercias = []\n",
    "\n",
    "for k in rango_k:\n",
    "    # Crear modelo KMeans con k clusters, random_state=42\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    \n",
    "    # Ajustar el modelo a los datos estandarizados\n",
    "    kmeans.fit(X_estandarizado)\n",
    "    \n",
    "    # Almacenar la inercia del modelo\n",
    "    inercias.append(kmeans.inertia_)\n",
    "\n",
    "# Visualizar metodo del codo\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rango_k, inercias, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Número de Clusters (k)', fontsize=12)\n",
    "plt.ylabel('Inercia (Suma de Distancias Cuadradas)', fontsize=12)\n",
    "plt.title('Método del Codo para Selección de k', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rango_k)\n",
    "plt.show()\n",
    "\n",
    "print(\"Inercias por número de clusters:\")\n",
    "for k, inercia in zip(rango_k, inercias):\n",
    "    print(f\"k={k}: Inercia = {inercia:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e22e3",
   "metadata": {},
   "source": [
    "### Ejercicio 2.4: Coeficiente de silueta\n",
    "\n",
    "El coeficiente de silueta cuantifica la calidad de clustering evaluando tanto cohesión interna (qué tan cerca están los puntos dentro de un cluster) como separación externa (qué tan alejados están de otros clusters). Valores cercanos a 1 indican clusters bien definidos y separados, mientras valores negativos sugieren asignaciones incorrectas. Esta métrica complementa el método del codo proporcionando validación cuantitativa de la estructura de clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb662b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular coeficiente de silueta para diferentes k\n",
    "coeficientes_silueta = []\n",
    "\n",
    "for k in rango_k:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    etiquetas = kmeans.fit_predict(X_estandarizado)\n",
    "    \n",
    "    # Calcular coeficiente de silueta usando silhouette_score\n",
    "    coef_silueta = silhouette_score(X_estandarizado, etiquetas)\n",
    "    coeficientes_silueta.append(coef_silueta)\n",
    "\n",
    "# Visualizar coeficientes de silueta\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rango_k, coeficientes_silueta, 'go-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Número de Clusters (k)', fontsize=12)\n",
    "plt.ylabel('Coeficiente de Silueta', fontsize=12)\n",
    "plt.title('Coeficiente de Silueta para Diferentes k', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rango_k)\n",
    "plt.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"Coeficientes de silueta por número de clusters:\")\n",
    "for k, coef in zip(rango_k, coeficientes_silueta):\n",
    "    print(f\"k={k}: Coeficiente de Silueta = {coef:.4f}\")\n",
    "\n",
    "mejor_k = rango_k[np.argmax(coeficientes_silueta)]\n",
    "print(f\"\\nMejor k según coeficiente de silueta: {mejor_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89365de7",
   "metadata": {},
   "source": [
    "### Ejercicio 2.5: Aplicación de K-Means con k óptimo\n",
    "\n",
    "La selección final del número de clusters balancea métricas cuantitativas (inercia, coeficiente de silueta) con interpretabilidad de negocio. En segmentación de clientes, un número moderado de clusters (3-5) facilita estrategias de marketing accionables, mientras que segmentaciones más granulares pueden carecer de diferenciación práctica entre grupos contiguos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43347ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar K-Means con k=3 (basado en generacion de datos y metricas)\n",
    "k_final = 3\n",
    "\n",
    "# Crear modelo KMeans final con k_final clusters\n",
    "kmeans_final = KMeans(n_clusters=k_final, random_state=42, n_init=10)\n",
    "\n",
    "# Ajustar el modelo y obtener etiquetas\n",
    "etiquetas_clusters = kmeans_final.fit_predict(X_estandarizado)\n",
    "\n",
    "# Agregar etiquetas al DataFrame original\n",
    "datos_clientes['cluster'] = etiquetas_clusters\n",
    "\n",
    "print(f\"Segmentación completada con k={k_final} clusters\")\n",
    "print(f\"\\nDistribución de clientes por cluster:\")\n",
    "print(datos_clientes['cluster'].value_counts().sort_index())\n",
    "print(f\"\\nPorcentaje por cluster:\")\n",
    "print((datos_clientes['cluster'].value_counts(normalize=True).sort_index() * 100).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b2924",
   "metadata": {},
   "source": [
    "### Ejercicio 2.6: Caracterización e interpretación de clusters\n",
    "\n",
    "La traducción de clusters algorítmicos a segmentos de negocio accionables requiere análisis de características promedio y asignación de etiquetas interpretables. Este proceso conecta resultados técnicos con estrategias de marketing, pricing, o gestión de inventario, transformando agrupaciones numéricas en insights operacionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e5d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular estadisticas promedio por cluster\n",
    "perfiles_clusters = datos_clientes.groupby('cluster')[variables_clustering].mean()\n",
    "\n",
    "print(\"Perfiles promedio por cluster:\\n\")\n",
    "print(perfiles_clusters.round(2))\n",
    "\n",
    "# Visualizar comparacion de clusters\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, var in enumerate(variables_clustering):\n",
    "    perfiles_clusters[var].plot(kind='bar', ax=axes[idx], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "    axes[idx].set_title(f'{var} por Cluster', fontsize=11)\n",
    "    axes[idx].set_xlabel('Cluster')\n",
    "    axes[idx].set_ylabel('Valor Promedio')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292213fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignar nombres interpretables a los clusters basados en caracteristicas\n",
    "nombres_clusters = {\n",
    "    0: 'Clientes Ocasionales',\n",
    "    1: 'Clientes Premium', \n",
    "    2: 'Clientes Regulares'\n",
    "}\n",
    "\n",
    "# Identificar cual cluster corresponde a cada perfil\n",
    "# El cluster con mayor valor_total es Premium, menor es Ocasional\n",
    "cluster_valores = perfiles_clusters['valor_total'].sort_values()\n",
    "\n",
    "# Reasignar nombres basados en valores\n",
    "if perfiles_clusters['valor_total'].iloc[0] > perfiles_clusters['valor_total'].iloc[2]:\n",
    "    if perfiles_clusters['valor_total'].iloc[0] > perfiles_clusters['valor_total'].iloc[1]:\n",
    "        nombres_clusters[0] = 'Clientes Premium'\n",
    "        if perfiles_clusters['valor_total'].iloc[1] > perfiles_clusters['valor_total'].iloc[2]:\n",
    "            nombres_clusters[1] = 'Clientes Regulares'\n",
    "            nombres_clusters[2] = 'Clientes Ocasionales'\n",
    "        else:\n",
    "            nombres_clusters[1] = 'Clientes Ocasionales'\n",
    "            nombres_clusters[2] = 'Clientes Regulares'\n",
    "\n",
    "datos_clientes['segmento'] = datos_clientes['cluster'].map(nombres_clusters)\n",
    "\n",
    "print(\"\\nInterpretación de segmentos:\")\n",
    "for cluster_id in range(k_final):\n",
    "    print(f\"\\n{nombres_clusters[cluster_id]} (Cluster {cluster_id}):\")\n",
    "    perfil = perfiles_clusters.loc[cluster_id]\n",
    "    print(f\"  - Frecuencia de compra: {perfil['frecuencia_compra']:.1f} transacciones/año\")\n",
    "    print(f\"  - Valor promedio: ${perfil['valor_promedio']:.2f} por transacción\")\n",
    "    print(f\"  - Valor total: ${perfil['valor_total']:.2f} anual\")\n",
    "    print(f\"  - Recency: {perfil['dias_ultima_compra']:.0f} días desde última compra\")\n",
    "    print(f\"  - Productos únicos: {perfil['productos_unicos']:.1f} productos diferentes\")\n",
    "    print(f\"  - Tasa de devolución: {perfil['tasa_devolucion']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar clusters en espacio 2D (usando dos variables principales)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Grafico 1: Frecuencia vs Valor Total\n",
    "plt.subplot(1, 2, 1)\n",
    "for cluster_id in range(k_final):\n",
    "    cluster_data = datos_clientes[datos_clientes['cluster'] == cluster_id]\n",
    "    plt.scatter(cluster_data['frecuencia_compra'], \n",
    "                cluster_data['valor_total'],\n",
    "                label=nombres_clusters[cluster_id],\n",
    "                alpha=0.6, s=50)\n",
    "\n",
    "plt.xlabel('Frecuencia de Compra')\n",
    "plt.ylabel('Valor Total ($)')\n",
    "plt.title('Segmentación: Frecuencia vs Valor Total')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Grafico 2: Dias Ultima Compra vs Valor Promedio\n",
    "plt.subplot(1, 2, 2)\n",
    "for cluster_id in range(k_final):\n",
    "    cluster_data = datos_clientes[datos_clientes['cluster'] == cluster_id]\n",
    "    plt.scatter(cluster_data['dias_ultima_compra'], \n",
    "                cluster_data['valor_promedio'],\n",
    "                label=nombres_clusters[cluster_id],\n",
    "                alpha=0.6, s=50)\n",
    "\n",
    "plt.xlabel('Días desde Última Compra')\n",
    "plt.ylabel('Valor Promedio ($)')\n",
    "plt.title('Segmentación: Recency vs Valor Promedio')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a58b9",
   "metadata": {},
   "source": [
    "## Parte 3: Clustering Jerárquico\n",
    "\n",
    "### Contexto del negocio\n",
    "\n",
    "Una plataforma de e-commerce necesita organizar su catálogo de 50 productos en categorías naturales sin usar clasificaciones predefinidas. El objetivo es descubrir jerarquías de similitud entre productos basándose en características de venta y comportamiento de clientes, permitiendo navegación intuitiva y recomendaciones efectivas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0db8e0",
   "metadata": {},
   "source": [
    "### Ejercicio 3.1: Generación de datos de productos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9b37a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset de productos\n",
    "datos_productos = pd.read_csv('data/catalogo_productos_retail.csv')\n",
    "\n",
    "print(f\"Dataset de productos cargado: {len(datos_productos)} productos\")\n",
    "print(f\"\\nEstadísticas descriptivas:\")\n",
    "print(datos_productos.describe().round(2))\n",
    "datos_productos.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df527478",
   "metadata": {},
   "source": [
    "### Ejercicio 3.2: Clustering jerárquico aglomerativo\n",
    "\n",
    "El clustering jerárquico construye una jerarquía de clusters mediante fusiones sucesivas, permitiendo explorar estructura de datos a múltiples niveles de granularidad. El dendrograma visualiza este proceso jerárquico, facilitando identificación de puntos de corte naturales y comparación de diferentes configuraciones de clusters sin pre-especificar k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dded47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para clustering jerarquico\n",
    "X_productos = datos_productos[['precio', 'ventas_mensuales', 'margen_beneficio', \n",
    "                                'rating_promedio', 'num_reviews']]\n",
    "\n",
    "# Estandarizar datos\n",
    "scaler_productos = StandardScaler()\n",
    "X_productos_std = scaler_productos.fit_transform(X_productos)\n",
    "\n",
    "# Calcular matriz de enlaces usando metodo de Ward\n",
    "# Ward minimiza la varianza intra-cluster\n",
    "Z = linkage(X_productos_std, method='ward')\n",
    "\n",
    "# Visualizar dendrograma\n",
    "plt.figure(figsize=(15, 7))\n",
    "dendrogram(Z, \n",
    "           labels=datos_productos['producto_id'].values,\n",
    "           leaf_font_size=8,\n",
    "           color_threshold=50)\n",
    "plt.title('Dendrograma de Clustering Jerárquico - Productos', fontsize=14)\n",
    "plt.xlabel('Producto ID', fontsize=12)\n",
    "plt.ylabel('Distancia (Ward)', fontsize=12)\n",
    "plt.axhline(y=50, color='r', linestyle='--', label='Corte sugerido (3 clusters)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Dendrograma generado usando método de Ward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9024be35",
   "metadata": {},
   "source": [
    "### Ejercicio 3.3: Comparación de métodos de enlace\n",
    "\n",
    "Diferentes métodos de enlace (single, complete, average, Ward) definen criterios distintos para medir distancia entre clusters, generando jerarquías con características específicas. Single linkage tiende a producir cadenas largas (sensible a outliers), complete genera clusters compactos pero puede fragmentar grupos naturales, average ofrece balance intermedio, y Ward optimiza homogeneidad intra-cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e3dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar diferentes metodos de enlace\n",
    "metodos = ['single', 'complete', 'average', 'ward']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, metodo in enumerate(metodos):\n",
    "    # Calcular linkage con cada metodo\n",
    "    Z_metodo = linkage(X_productos_std, method=metodo)\n",
    "    \n",
    "    # Visualizar dendrograma\n",
    "    dendrogram(Z_metodo, \n",
    "               ax=axes[idx],\n",
    "               labels=datos_productos['producto_id'].values,\n",
    "               leaf_font_size=6,\n",
    "               no_labels=(len(datos_productos) > 30))\n",
    "    \n",
    "    axes[idx].set_title(f'Método: {metodo.upper()}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Producto ID', fontsize=10)\n",
    "    axes[idx].set_ylabel('Distancia', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Comparación de métodos de enlace:\")\n",
    "print(\"- Single: Minimiza distancia entre elementos más cercanos (sensible a outliers)\")\n",
    "print(\"- Complete: Maximiza distancia entre elementos más lejanos (clusters compactos)\")\n",
    "print(\"- Average: Promedia distancias (compromiso entre single y complete)\")\n",
    "print(\"- Ward: Minimiza varianza intra-cluster (clusters balanceados)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129b4d24",
   "metadata": {},
   "source": [
    "### Ejercicio 3.4: Corte del dendrograma y caracterización\n",
    "\n",
    "El corte del dendrograma a una altura específica define el número final de clusters. Este punto de corte se selecciona evaluando la estructura del dendrograma (ramas largas verticales indican separación natural entre grupos) y balanceando granularidad con interpretabilidad de negocio. La caracterización posterior asigna significado a cada cluster identificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6aaf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar AgglomerativeClustering con n_clusters=3\n",
    "modelo_jerarquico = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "etiquetas_productos = modelo_jerarquico.fit_predict(X_productos_std)\n",
    "\n",
    "# Agregar etiquetas al dataset\n",
    "datos_productos['categoria'] = etiquetas_productos\n",
    "\n",
    "# Analizar caracteristicas por categoria\n",
    "print(\"Distribución de productos por categoría:\")\n",
    "print(datos_productos['categoria'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Características promedio por categoría:\\n\")\n",
    "perfiles_productos = datos_productos.groupby('categoria')[['precio', 'ventas_mensuales', \n",
    "                                                             'margen_beneficio', 'rating_promedio', \n",
    "                                                             'num_reviews']].mean()\n",
    "print(perfiles_productos.round(2))\n",
    "\n",
    "# Asignar nombres interpreables\n",
    "nombres_categorias = {}\n",
    "for cat in range(3):\n",
    "    perfil = perfiles_productos.loc[cat]\n",
    "    if perfil['precio'] > 400:\n",
    "        nombres_categorias[cat] = 'Electrónica'\n",
    "    elif perfil['precio'] < 30:\n",
    "        nombres_categorias[cat] = 'Accesorios'\n",
    "    else:\n",
    "        nombres_categorias[cat] = 'Ropa y Textiles'\n",
    "\n",
    "datos_productos['categoria_nombre'] = datos_productos['categoria'].map(nombres_categorias)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Categorías identificadas:\")\n",
    "for cat, nombre in nombres_categorias.items():\n",
    "    print(f\"\\nCategoría {cat}: {nombre}\")\n",
    "    perfil = perfiles_productos.loc[cat]\n",
    "    print(f\"  - Precio promedio: ${perfil['precio']:.2f}\")\n",
    "    print(f\"  - Ventas mensuales: {perfil['ventas_mensuales']:.0f} unidades\")\n",
    "    print(f\"  - Margen de beneficio: {perfil['margen_beneficio']:.1f}%\")\n",
    "    print(f\"  - Rating: {perfil['rating_promedio']:.2f}/5.0\")\n",
    "    print(f\"  - Reviews: {perfil['num_reviews']:.0f} promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9925f5d",
   "metadata": {},
   "source": [
    "## Parte 4: Reducción de Dimensionalidad con PCA\n",
    "\n",
    "### Contexto del negocio\n",
    "\n",
    "Un equipo de analítica tiene un dataset con 20 variables que describen el comportamiento de usuarios en una plataforma digital. Para facilitar la visualización, comprensión de patrones, y acelerar modelos posteriores, se requiere reducir la dimensionalidad preservando la mayor cantidad de información posible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2423e37",
   "metadata": {},
   "source": [
    "### Ejercicio 4.1: Generación de datos de alta dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c53ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset de alta dimensionalidad\n",
    "df_alta_dim = pd.read_csv('data/metricas_usuarios_plataforma_digital.csv')\n",
    "\n",
    "n_variables = df_alta_dim.shape[1] - 1  # Excluir grupo_real\n",
    "\n",
    "print(f\"Dataset cargado: {df_alta_dim.shape[0]} observaciones, {n_variables} variables\")\n",
    "print(f\"\\nPrimeras filas:\")\n",
    "print(df_alta_dim.head())\n",
    "print(f\"\\nDistribución de grupos reales:\")\n",
    "print(df_alta_dim['grupo_real'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36a766",
   "metadata": {},
   "source": [
    "### Ejercicio 4.2: Aplicación de PCA\n",
    "\n",
    "PCA (Principal Component Analysis) identifica direcciones de máxima varianza en datos de alta dimensionalidad, proyectando observaciones a un espacio de menor dimensión que preserva la mayor cantidad posible de información. La varianza explicada por cada componente cuantifica qué proporción de la información original se retiene, permitiendo selección informada del número de componentes para balance entre reducción dimensional y pérdida de información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dc0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos (excluir grupo_real)\n",
    "X_pca = df_alta_dim.drop('grupo_real', axis=1)\n",
    "nombres_vars = X_pca.columns.tolist()\n",
    "\n",
    "# Estandarizar datos\n",
    "scaler_pca = StandardScaler()\n",
    "X_pca_std = scaler_pca.fit_transform(X_pca)\n",
    "\n",
    "# Aplicar PCA con todos los componentes\n",
    "pca = PCA()\n",
    "X_pca_transformado = pca.fit_transform(X_pca_std)\n",
    "\n",
    "# Analizar varianza explicada\n",
    "varianza_explicada = pca.explained_variance_ratio_\n",
    "varianza_acumulada = np.cumsum(varianza_explicada)\n",
    "n_variables = len(nombres_vars)\n",
    "\n",
    "print(\"Varianza explicada por cada componente principal:\")\n",
    "for i, var in enumerate(varianza_explicada[:10], 1):\n",
    "    print(f\"PC{i}: {var*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Varianza acumulada por primeros componentes:\")\n",
    "for i in [2, 3, 5, 10]:\n",
    "    print(f\"Primeros {i} componentes: {varianza_acumulada[i-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a56602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar scree plot y varianza acumulada\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].plot(range(1, len(varianza_explicada)+1), varianza_explicada, 'bo-')\n",
    "axes[0].set_xlabel('Componente Principal', fontsize=11)\n",
    "axes[0].set_ylabel('Proporción de Varianza Explicada', fontsize=11)\n",
    "axes[0].set_title('Scree Plot', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0.05, color='r', linestyle='--', alpha=0.5, label='5% umbral')\n",
    "axes[0].legend()\n",
    "\n",
    "# Varianza acumulada\n",
    "axes[1].plot(range(1, len(varianza_acumulada)+1), varianza_acumulada, 'go-')\n",
    "axes[1].axhline(y=0.80, color='r', linestyle='--', alpha=0.5, label='80% varianza')\n",
    "axes[1].axhline(y=0.90, color='orange', linestyle='--', alpha=0.5, label='90% varianza')\n",
    "axes[1].set_xlabel('Número de Componentes', fontsize=11)\n",
    "axes[1].set_ylabel('Varianza Acumulada', fontsize=11)\n",
    "axes[1].set_title('Varianza Acumulada', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determinar numero de componentes para 80% y 90% de varianza\n",
    "n_comp_80 = np.argmax(varianza_acumulada >= 0.80) + 1\n",
    "n_comp_90 = np.argmax(varianza_acumulada >= 0.90) + 1\n",
    "\n",
    "print(f\"\\nComponentes necesarios para 80% varianza: {n_comp_80}\")\n",
    "print(f\"Componentes necesarios para 90% varianza: {n_comp_90}\")\n",
    "print(f\"Reducción dimensional: {n_variables} → {n_comp_80} variables ({n_comp_80/n_variables*100:.1f}% del tamaño original)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63035a35",
   "metadata": {},
   "source": [
    "### Ejercicio 4.3: Visualización en 2D con PCA\n",
    "\n",
    "La proyección a 2 dimensiones mediante PCA facilita visualización de estructura de datos originalmente en alta dimensionalidad. Los dos primeros componentes principales capturan las direcciones de mayor variabilidad, permitiendo identificación visual de clusters, outliers, y patrones que serían imposibles de observar en el espacio original de 20+ variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar PCA con 2 componentes\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_pca_std)\n",
    "\n",
    "# Crear DataFrame con componentes\n",
    "df_pca_2d = pd.DataFrame(X_pca_2d, columns=['PC1', 'PC2'])\n",
    "df_pca_2d['grupo'] = df_alta_dim['grupo_real']\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(10, 7))\n",
    "colores = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "grupos_nombres = ['Grupo A', 'Grupo B', 'Grupo C']\n",
    "\n",
    "for grupo_id in range(3):\n",
    "    mask = df_pca_2d['grupo'] == grupo_id\n",
    "    plt.scatter(df_pca_2d.loc[mask, 'PC1'], \n",
    "                df_pca_2d.loc[mask, 'PC2'],\n",
    "                c=colores[grupo_id],\n",
    "                label=grupos_nombres[grupo_id],\n",
    "                alpha=0.6,\n",
    "                s=50,\n",
    "                edgecolors='black',\n",
    "                linewidth=0.5)\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% varianza)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% varianza)', fontsize=12)\n",
    "plt.title('Proyección PCA 2D - Visualización de Grupos', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Varianza total capturada por 2 componentes: {sum(pca_2d.explained_variance_ratio_)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994285b4",
   "metadata": {},
   "source": [
    "### Ejercicio 4.4: Análisis de loadings\n",
    "\n",
    "Los loadings (o pesos factoriales) de PCA revelan cómo cada variable original contribuye a los componentes principales. Variables con loadings altos (en valor absoluto) tienen mayor influencia en la dirección del componente, permitiendo interpretación temática de componentes. Por ejemplo, si PC1 tiene loadings altos en variables de gasto, puede interpretarse como \"dimensión de valor económico\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc27372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer loadings (componentes)\n",
    "loadings = pca_2d.components_.T * np.sqrt(pca_2d.explained_variance_)\n",
    "\n",
    "# Crear DataFrame de loadings\n",
    "df_loadings = pd.DataFrame(\n",
    "    loadings,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=nombres_vars\n",
    ")\n",
    "\n",
    "print(\"Loadings de variables en los dos primeros componentes:\\n\")\n",
    "print(df_loadings.round(3))\n",
    "\n",
    "# Visualizar loadings\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Loadings PC1\n",
    "df_loadings['PC1'].sort_values().plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Loadings - Componente Principal 1', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Loading', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Loadings PC2\n",
    "df_loadings['PC2'].sort_values().plot(kind='barh', ax=axes[1], color='coral')\n",
    "axes[1].set_title('Loadings - Componente Principal 2', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Loading', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identificar variables mas influyentes\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Variables con mayor influencia en PC1:\")\n",
    "print(df_loadings['PC1'].abs().sort_values(ascending=False).head(5))\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Variables con mayor influencia en PC2:\")\n",
    "print(df_loadings['PC2'].abs().sort_values(ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0808534c",
   "metadata": {},
   "source": [
    "## Parte 5: Técnicas Avanzadas de Reducción de Dimensionalidad\n",
    "\n",
    "### Contexto\n",
    "\n",
    "PCA es una técnica lineal que puede no capturar relaciones no lineales complejas en los datos. Técnicas modernas como t-SNE y UMAP están diseñadas para preservar estructura local y global, siendo especialmente útiles para visualización de datos con patrones complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e8d072",
   "metadata": {},
   "source": [
    "### Ejercicio 5.1: Aplicación de t-SNE\n",
    "\n",
    "t-SNE (t-Distributed Stochastic Neighbor Embedding) es una técnica no-lineal que preserva estructura local de datos, proyectando observaciones similares cerca unas de otras en el espacio reducido. A diferencia de PCA que busca varianza máxima globalmente, t-SNE optimiza para mantener vecindarios locales, revelando clusters y patrones no-lineales que métodos lineales pueden no capturar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56816fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar t-SNE con perplexity=30\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_pca_std)\n",
    "\n",
    "# Crear DataFrame\n",
    "df_tsne = pd.DataFrame(X_tsne, columns=['t-SNE1', 't-SNE2'])\n",
    "df_tsne['grupo'] = df_alta_dim['grupo_real']\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for grupo_id in range(3):\n",
    "    mask = df_tsne['grupo'] == grupo_id\n",
    "    plt.scatter(df_tsne.loc[mask, 't-SNE1'], \n",
    "                df_tsne.loc[mask, 't-SNE2'],\n",
    "                c=colores[grupo_id],\n",
    "                label=grupos_nombres[grupo_id],\n",
    "                alpha=0.6,\n",
    "                s=50,\n",
    "                edgecolors='black',\n",
    "                linewidth=0.5)\n",
    "\n",
    "plt.xlabel('t-SNE Dimensión 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimensión 2', fontsize=12)\n",
    "plt.title('Proyección t-SNE 2D - Visualización de Grupos', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"t-SNE aplicado correctamente\")\n",
    "print(\"Nota: t-SNE preserva estructura local, grupos cercanos en espacio original permanecen cercanos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39805be",
   "metadata": {},
   "source": [
    "### Ejercicio 5.2: Comparación visual PCA vs t-SNE\n",
    "\n",
    "La comparación entre PCA y t-SNE ilustra fortalezas de cada enfoque. PCA, siendo lineal, es rápida, determinística, e interpretable mediante varianza explicada y loadings. t-SNE, siendo no-lineal, puede revelar estructura local compleja pero es computacionalmente costosa, estocástica (resultados varían con random seed), y no permite interpretación directa de ejes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparacion lado a lado\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Grafico PCA\n",
    "for grupo_id in range(3):\n",
    "    mask = df_pca_2d['grupo'] == grupo_id\n",
    "    axes[0].scatter(df_pca_2d.loc[mask, 'PC1'], \n",
    "                    df_pca_2d.loc[mask, 'PC2'],\n",
    "                    c=colores[grupo_id],\n",
    "                    label=grupos_nombres[grupo_id],\n",
    "                    alpha=0.6,\n",
    "                    s=50,\n",
    "                    edgecolors='black',\n",
    "                    linewidth=0.5)\n",
    "\n",
    "axes[0].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}%)', fontsize=11)\n",
    "axes[0].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}%)', fontsize=11)\n",
    "axes[0].set_title('PCA: Reducción Lineal', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Grafico t-SNE\n",
    "for grupo_id in range(3):\n",
    "    mask = df_tsne['grupo'] == grupo_id\n",
    "    axes[1].scatter(df_tsne.loc[mask, 't-SNE1'], \n",
    "                    df_tsne.loc[mask, 't-SNE2'],\n",
    "                    c=colores[grupo_id],\n",
    "                    label=grupos_nombres[grupo_id],\n",
    "                    alpha=0.6,\n",
    "                    s=50,\n",
    "                    edgecolors='black',\n",
    "                    linewidth=0.5)\n",
    "\n",
    "axes[1].set_xlabel('t-SNE Dimensión 1', fontsize=11)\n",
    "axes[1].set_ylabel('t-SNE Dimensión 2', fontsize=11)\n",
    "axes[1].set_title('t-SNE: Preservación de Estructura Local', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Comparación PCA vs t-SNE:\")\n",
    "print(\"- PCA: Técnica lineal, maximiza varianza, interpretable, rápida\")\n",
    "print(\"- t-SNE: Técnica no-lineal, preserva estructura local, mejor para visualización\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c9608a",
   "metadata": {},
   "source": [
    "## Parte 6: Síntesis y Aplicación Integrada\n",
    "\n",
    "### Ejercicio Final: Pipeline completo de análisis no supervisado\n",
    "\n",
    "**Contexto:** Un banco desea segmentar su cartera de préstamos hipotecarios para identificar perfiles de riesgo y optimizar estrategias de cobro. El dataset contiene información de 500 préstamos con 15 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b5dd1",
   "metadata": {},
   "source": [
    "### Ejercicio 6.1: Análisis exploratorio de datos bancarios\n",
    "\n",
    "El análisis de cartera de préstamos hipotecarios requiere identificación de perfiles de riesgo mediante combinación de variables financieras (LTV ratio, score crediticio, ratio deuda-ingreso) y demográficas (edad, antigüedad laboral). La segmentación permite estratificación de riesgo para ajuste de tasas, asignación de recursos de cobranza, y decisiones de aprobación de nuevos préstamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da24f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset de préstamos\n",
    "datos_prestamos = pd.read_csv('data/cartera_prestamos_hipotecarios.csv')\n",
    "\n",
    "print(f\"Dataset de préstamos cargado: {len(datos_prestamos)} préstamos\")\n",
    "print(f\"\\nEstadísticas descriptivas:\")\n",
    "print(datos_prestamos.describe().round(2))\n",
    "datos_prestamos.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a145cd",
   "metadata": {},
   "source": [
    "### Ejercicio 6.2: Pipeline integrado de análisis\n",
    "\n",
    "Un pipeline PCA + K-Means combina reducción de dimensionalidad con clustering. PCA elimina ruido y multicolinealidad, comprimiendo información correlacionada en componentes ortogonales. K-Means sobre componentes principales puede mejorar calidad de clusters y velocidad computacional. La comparación con clustering directo evalúa trade-off entre compresión de información y calidad de segmentación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e94df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "variables_prestamos = ['monto_prestamo', 'ingreso_anual', 'ltv_ratio', 'score_credito',\n",
    "                       'tasa_interes', 'plazo_anos', 'edad_solicitante', 'antiguedad_empleo',\n",
    "                       'num_dependientes', 'deuda_total', 'pagos_atrasados_12m',\n",
    "                       'ratio_deuda_ingreso', 'cuota_mensual']\n",
    "\n",
    "X_prestamos = datos_prestamos[variables_prestamos]\n",
    "\n",
    "# Estandarizar datos\n",
    "scaler_prestamos = StandardScaler()\n",
    "X_prestamos_std = scaler_prestamos.fit_transform(X_prestamos)\n",
    "\n",
    "# Metodo 1: K-Means directo sobre datos estandarizados\n",
    "kmeans_directo = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "etiquetas_directo = kmeans_directo.fit_predict(X_prestamos_std)\n",
    "\n",
    "# Metodo 2: PCA seguido de K-Means\n",
    "# Primero aplicar PCA para reducir a 5 componentes (capturan ~80% varianza)\n",
    "pca_prestamos = PCA(n_components=5)\n",
    "X_pca_prestamos = pca_prestamos.fit_transform(X_prestamos_std)\n",
    "\n",
    "kmeans_pca = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "etiquetas_pca = kmeans_pca.fit_predict(X_pca_prestamos)\n",
    "\n",
    "# Agregar resultados al dataset\n",
    "datos_prestamos['cluster_directo'] = etiquetas_directo\n",
    "datos_prestamos['cluster_pca'] = etiquetas_pca\n",
    "\n",
    "# Comparar resultados\n",
    "print(\"Método 1: K-Means sobre datos originales (13 variables)\")\n",
    "print(f\"Inercia: {kmeans_directo.inertia_:.2f}\")\n",
    "print(f\"Coeficiente de silueta: {silhouette_score(X_prestamos_std, etiquetas_directo):.4f}\")\n",
    "print(f\"Distribución: {pd.Series(etiquetas_directo).value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Método 2: K-Means sobre PCA (5 componentes)\")\n",
    "print(f\"Varianza explicada por PCA: {sum(pca_prestamos.explained_variance_ratio_)*100:.2f}%\")\n",
    "print(f\"Inercia: {kmeans_pca.inertia_:.2f}\")\n",
    "print(f\"Coeficiente de silueta: {silhouette_score(X_pca_prestamos, etiquetas_pca):.4f}\")\n",
    "print(f\"Distribución: {pd.Series(etiquetas_pca).value_counts().sort_index().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc020753",
   "metadata": {},
   "source": [
    "### Ejercicio 6.3: Caracterización de perfiles de riesgo y recomendaciones\n",
    "\n",
    "La traducción de clusters a perfiles de riesgo operacionales requiere análisis multivariado de indicadores financieros y construcción de scores compuestos. Cada perfil debe asociarse con estrategias diferenciadas de gestión: clientes bajo riesgo reciben beneficios (tasas preferenciales, productos adicionales), riesgo medio requiere monitoreo proactivo, y alto riesgo demanda intervención intensiva y reestructuración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b595177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar perfiles usando metodo directo\n",
    "perfiles_riesgo = datos_prestamos.groupby('cluster_directo')[variables_prestamos].mean()\n",
    "\n",
    "print(\"PERFILES DE RIESGO IDENTIFICADOS\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Determinar nivel de riesgo por cluster basado en indicadores clave\n",
    "for cluster_id in range(3):\n",
    "    perfil = perfiles_riesgo.loc[cluster_id]\n",
    "    \n",
    "    # Calcular score de riesgo compuesto\n",
    "    riesgo_score = (\n",
    "        (perfil['ltv_ratio'] / 100) * 0.3 +  # Mayor LTV = mayor riesgo\n",
    "        ((800 - perfil['score_credito']) / 800) * 0.3 +  # Menor score = mayor riesgo\n",
    "        (perfil['ratio_deuda_ingreso'] / 100) * 0.2 +  # Mayor ratio = mayor riesgo\n",
    "        (perfil['pagos_atrasados_12m'] / 5) * 0.2  # Más atrasos = mayor riesgo\n",
    "    )\n",
    "    \n",
    "    if riesgo_score < 0.4:\n",
    "        nivel = \"BAJO RIESGO\"\n",
    "    elif riesgo_score < 0.6:\n",
    "        nivel = \"RIESGO MEDIO\"\n",
    "    else:\n",
    "        nivel = \"ALTO RIESGO\"\n",
    "    \n",
    "    print(f\"\\nCLUSTER {cluster_id}: {nivel}\")\n",
    "    print(f\"Score de Riesgo Compuesto: {riesgo_score:.3f}\")\n",
    "    print(f\"  Tamaño del segmento: {(datos_prestamos['cluster_directo']==cluster_id).sum()} préstamos\")\n",
    "    print(f\"  Monto promedio: ${perfil['monto_prestamo']:,.0f}\")\n",
    "    print(f\"  Ingreso promedio: ${perfil['ingreso_anual']:,.0f}\")\n",
    "    print(f\"  LTV Ratio: {perfil['ltv_ratio']:.1f}%\")\n",
    "    print(f\"  Score de crédito: {perfil['score_credito']:.0f}\")\n",
    "    print(f\"  Tasa de interés: {perfil['tasa_interes']:.2f}%\")\n",
    "    print(f\"  Ratio deuda/ingreso: {perfil['ratio_deuda_ingreso']:.1f}%\")\n",
    "    print(f\"  Pagos atrasados (12m): {perfil['pagos_atrasados_12m']:.2f}\")\n",
    "    print(f\"  Antigüedad empleo: {perfil['antiguedad_empleo']:.1f} años\")\n",
    "    \n",
    "    # Generar recomendaciones especificas\n",
    "    print(f\"\\n  RECOMENDACIONES:\")\n",
    "    if nivel == \"BAJO RIESGO\":\n",
    "        print(\"    • Mantener seguimiento estándar\")\n",
    "        print(\"    • Ofrecer productos adicionales (cross-selling)\")\n",
    "        print(\"    • Considerar tasas preferenciales para refinanciamiento\")\n",
    "    elif nivel == \"RIESGO MEDIO\":\n",
    "        print(\"    • Monitoreo mensual de pagos\")\n",
    "        print(\"    • Ofrecer asesoría financiera proactiva\")\n",
    "        print(\"    • Evaluar refinanciamiento si mejoran condiciones\")\n",
    "    else:\n",
    "        print(\"    • Seguimiento intensivo (semanal)\")\n",
    "        print(\"    • Contacto proactivo ante primer atraso\")\n",
    "        print(\"    • Programa de reestructuración de deuda\")\n",
    "        print(\"    • Evaluar garantías adicionales\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ff565",
   "metadata": {},
   "source": [
    "## Conclusiones del Laboratorio\n",
    "\n",
    "Este laboratorio ha cubierto las técnicas fundamentales de aprendizaje no supervisado aplicadas a casos de negocio reales:\n",
    "\n",
    "### Aprendizajes Clave\n",
    "\n",
    "1. **K-Means para Segmentación**: Técnica eficiente para identificar grupos naturales en datos de clientes, productos o transacciones. La selección del número de clusters requiere balance entre métricas cuantitativas (método del codo, silueta) y utilidad de negocio.\n",
    "\n",
    "2. **Clustering Jerárquico**: Permite explorar estructura de datos a múltiples niveles de granularidad. Los dendrogramas facilitan visualización de relaciones y la comparación de métodos de enlace revela diferentes perspectivas sobre similitud.\n",
    "\n",
    "3. **PCA para Reducción de Dimensionalidad**: Técnica lineal que comprime información preservando varianza. Útil para visualización, preprocesamiento, y comprensión de relaciones entre variables mediante análisis de loadings.\n",
    "\n",
    "4. **Técnicas Avanzadas (t-SNE)**: Métodos no-lineales que preservan estructura local, superiores a PCA para visualización de patrones complejos, aunque menos interpretables y más costosos computacionalmente.\n",
    "\n",
    "5. **Pipeline Integrado**: La combinación de reducción de dimensionalidad con clustering puede mejorar resultados al eliminar ruido y reducir complejidad computacional.\n",
    "\n",
    "### Consideraciones para Producción\n",
    "\n",
    "- **Estandarización**: Crítica para algoritmos basados en distancias\n",
    "- **Selección de variables**: Incluir solo variables relevantes mejora resultados\n",
    "- **Interpretabilidad**: Traducir resultados técnicos a insights accionables de negocio\n",
    "- **Validación**: Colaborar con expertos de dominio para validar segmentaciones\n",
    "- **Monitoreo**: Re-entrenar periódicamente conforme evolucionan los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6bda1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
